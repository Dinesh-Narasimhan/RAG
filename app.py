# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w7EIbkIlWwREXQtuKom7BF7cibbkEEmn
"""

# -*- coding: utf-8 -*-
import streamlit as st
from sentence_transformers import SentenceTransformer
from transformers import T5Tokenizer, T5ForConditionalGeneration
import torch
import numpy as np
import faiss
import os
import docx
import fitz
import tempfile

# Load models once
@st.cache_resource
def load_models():
    embedder = SentenceTransformer('all-MiniLM-L6-v2')
    tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-small", cache_dir="models")
    model = T5ForConditionalGeneration.from_pretrained("google/flan-t5-small", cache_dir="models").to("cpu")
    return embedder, tokenizer, model

embedder, tokenizer, model = load_models()

def read_file(file_path, ext):
    if ext == ".txt":
        return open(file_path, 'r', encoding='utf-8').read()
    elif ext == ".docx":
        doc = docx.Document(file_path)
        return "\n".join([p.text for p in doc.paragraphs if p.text.strip()])
    elif ext == ".pdf":
        pdf = fitz.open(file_path)
        return "\n".join([page.get_text() for page in pdf])
    else:
        return ""

def chunk_text(text, max_words=100):
    words = text.split()
    return [' '.join(words[i:i + max_words]) for i in range(0, len(words), max_words)]

st.title("üìö Ask Your Notes ‚Äî Fast & Free (Flan-T5)")

uploaded_file = st.file_uploader("üìÇ Upload your .txt, .docx, or .pdf", type=["txt", "docx", "pdf"])

if uploaded_file:
    with tempfile.NamedTemporaryFile(delete=False) as tmp:
        tmp.write(uploaded_file.read())
        file_path = tmp.name

    ext = os.path.splitext(uploaded_file.name)[-1].lower()
    full_text = read_file(file_path, ext)

    chunks = chunk_text(full_text)
    st.success(f"‚úÖ Text split into {len(chunks)} chunks")

    embeddings = embedder.encode(chunks)
    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(np.array(embeddings))

    question = st.text_input("‚ùì Ask your question:")
    if question:
        question_embedding = embedder.encode([question])
        D, I = index.search(np.array(question_embedding), k=1)
        retrieved_chunk = chunks[I[0][0]]

        prompt = f"Answer this question using the context: {retrieved_chunk}\nQuestion: {question}"

        input_ids = tokenizer(prompt, return_tensors="pt").input_ids
        with torch.no_grad():
            outputs = model.generate(input_ids, max_new_tokens=300)
        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)

        st.markdown("### üí° Answer")
        st.write(answer)
