# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w7EIbkIlWwREXQtuKom7BF7cibbkEEmn
"""

# -*- coding: utf-8 -*-
import streamlit as st
from transformers import AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import torch
from docx import Document
import PyPDF2

st.set_page_config(page_title="RAG Tutor", layout="wide")
st.title("üìò ASK YOUR NOTES")
st.markdown("Upload your document and ask questions.")

uploaded_file = st.file_uploader("Upload a .txt, .pdf, or .docx file", type=["txt", "pdf", "docx"])
if uploaded_file:
    if uploaded_file.name.endswith(".txt"):
        text = uploaded_file.read().decode("utf-8", errors="ignore")
    elif uploaded_file.name.endswith(".pdf"):
        reader = PyPDF2.PdfReader(uploaded_file)
        text = "\n".join([p.extract_text() or "" for p in reader.pages])
    else:
        doc = Document(uploaded_file)
        text = "\n".join([para.text for para in doc.paragraphs])

    chunks = [" ".join(text.split()[i:i+100]) for i in range(0, len(text.split()), 100)]
    embedder = SentenceTransformer("all-MiniLM-L6-v2")
    embeds = embedder.encode(chunks)
    faiss_index = faiss.IndexFlatL2(embeds.shape[1])
    faiss_index.add(np.array(embeds))
    st.success(f"Indexed {len(chunks)} chunks.")

    @st.cache_resource
    def load_tinyllama():
        tokenizer = AutoTokenizer.from_pretrained("TinyLlama/TinyLlama-1.1B-Chat-v1.0")
        model = AutoModelForCausalLM.from_pretrained(
            "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            torch_dtype=torch.float32,
            device_map="auto"
        )
        return tokenizer, model

    tokenizer, model = load_tinyllama()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    user_input = st.text_input("üí¨ Ask a question")
    if st.button("üîç Get Answer") and user_input:
        q_emb = embedder.encode([user_input])
        _, I = faiss_index.search(np.array(q_emb), k=1)
        ctx = chunks[I[0][0]]

        prompt = f"Context: {ctx}\nQuestion: {user_input}\nAnswer:"
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512).to(device)
        outputs = model.generate(**inputs, max_new_tokens=300, temperature=0.7)
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)[len(prompt):].strip()
        st.session_state.chat_history.append((user_input, response))

    for q, a in st.session_state.chat_history[::-1]:
        st.chat_message("user").markdown(f"**You:** {q}")
        st.chat_message("assistant").markdown(f"**TinyLlama:** {a}")
