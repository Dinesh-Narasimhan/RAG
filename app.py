# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w7EIbkIlWwREXQtuKom7BF7cibbkEEmn
"""

# -*- coding: utf-8 -*-
import streamlit as st
from transformers import AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import torch
import docx as docx_module
from docx import Document
import PyPDF2

# ------------------------ Setup ------------------------
st.set_page_config(page_title="RAG Tutor", layout="wide")
st.title("üìò ASK YOUR NOTES ")
st.markdown("Ask questions from your uploaded document.")

# ------------------------ File Upload ------------------------
uploaded_file = st.file_uploader("Upload a .txt, .pdf, or .docx file", type=["txt", "pdf", "docx"])
if uploaded_file:
    ext = uploaded_file.name.split('.')[-1]
    raw_text = ""

    if ext == "txt":
        raw_text = uploaded_file.read().decode("utf-8", errors="ignore")
    elif ext == "pdf":
        reader = PyPDF2.PdfReader(uploaded_file)
        raw_text = "\n".join([page.extract_text() or "" for page in reader.pages])
    elif ext == "docx":
        doc = Document(uploaded_file)
        raw_text = "\n".join([para.text for para in doc.paragraphs])

    # ------------------------ Chunking ------------------------
    def chunk_text(text, max_words=100):
        words = text.split()
        return [' '.join(words[i:i+max_words]) for i in range(0, len(words), max_words)]

    chunks = chunk_text(raw_text)

    # ------------------------ Embedding ------------------------
    embedder = SentenceTransformer("all-MiniLM-L6-v2")
    chunk_embeddings = embedder.encode(chunks)
    dimension = chunk_embeddings.shape[1]

    # Build FAISS index
    faiss_index = faiss.IndexFlatL2(dimension)
    faiss_index.add(np.array(chunk_embeddings))

    st.success(f"‚úÖ Loaded and indexed {len(chunks)} text chunks.")

    # ------------------------ Load Phi-1.5 Safely ------------------------
    @st.cache_resource
    def load_phi15():
        tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-1_5")
        model = AutoModelForCausalLM.from_pretrained("microsoft/phi-1_5")
        if torch.cuda.is_available():
            model = model.to(torch.float16).to("cuda")
        else:
            model = model.float().to("cpu")
        return tokenizer, model

    tokenizer, model = load_phi15()
    device = "cuda" if torch.cuda.is_available() else "cpu"

    # ------------------------ Chat History ------------------------
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    # ------------------------ User Input + QA ------------------------
    user_input = st.text_input("üí¨ Ask a question", placeholder="e.g., What is deep learning?")
    if st.button("üîç Get Answer") and user_input.strip() != "":
        # Semantic search
        q_embedding = embedder.encode([user_input])
        D, I = faiss_index.search(np.array(q_embedding), k=1)
        context = chunks[I[0][0]]

        # Build prompt
        prompt = f"""You are a helpful tutor. Using the context provided, write a detailed, clear answer to the question. Make sure it's at least 300 words long.

Context: {context}

Question: {user_input}
Answer:"""

        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512).to(device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=700,
                min_length=450,
                do_sample=True,
                top_k=50,
                top_p=0.95,
                temperature=0.7,
                eos_token_id=tokenizer.eos_token_id
            )

        response = tokenizer.decode(outputs[0], skip_special_tokens=True)[len(prompt):].strip()
        st.session_state.chat_history.append((user_input, response))

    # ------------------------ Display Chat ------------------------
    for q, a in st.session_state.chat_history[::-1]:
        with st.chat_message("user"):
            st.markdown(f"**You:** {q}")
        with st.chat_message("assistant"):
            st.markdown(f"**Phi-1.5:** {a}")
