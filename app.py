# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w7EIbkIlWwREXQtuKom7BF7cibbkEEmn
"""

# -*- coding: utf-8 -*-
import streamlit as st
from transformers import AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import tempfile
import os
import torch
import docx
import PyPDF2

# ------------------------ Setup ------------------------
st.set_page_config(page_title="RAG Tutor", layout="wide")
st.title("üìò ASK YOUR NOTES")
st.markdown("Ask questions from your uploaded document.")

# ------------------------ File Upload ------------------------
uploaded_file = st.file_uploader("Upload a .txt, .pdf, or .docx file", type=["txt", "pdf", "docx"])
if uploaded_file:
    ext = uploaded_file.name.split('.')[-1]
    raw_text = ""

    if ext == "txt":
        raw_text = uploaded_file.read().decode("utf-8", errors="ignore")
    elif ext == "pdf":
        reader = PyPDF2.PdfReader(uploaded_file)
        raw_text = "\n".join([page.extract_text() for page in reader.pages if page.extract_text()])
    elif ext == "docx":
        doc = docx.Document(uploaded_file)
        raw_text = "\n".join([para.text for para in doc.paragraphs])

    # Chunking
    def chunk_text(text, max_words=100):
        words = text.split()
        return [' '.join(words[i:i+max_words]) for i in range(0, len(words), max_words)]

    chunks = chunk_text(raw_text)

    # Embedding
    embedder = SentenceTransformer("all-MiniLM-L6-v2")
    chunk_embeddings = embedder.encode(chunks)
    dimension = chunk_embeddings.shape[1]

    # Build FAISS index
    faiss_index = faiss.IndexFlatL2(dimension)
    faiss_index.add(np.array(chunk_embeddings))

    st.success(f"‚úÖ Loaded and indexed {len(chunks)} text chunks.")

    # Load TinyLlama (works on Streamlit Cloud Free)
    @st.cache_resource
    def load_tinyllama():
        tokenizer = AutoTokenizer.from_pretrained("TinyLlama/TinyLlama-1.1B-Chat-v1.0")
        model = AutoModelForCausalLM.from_pretrained(
            "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            torch_dtype=torch.float32
        )
        return tokenizer, model

    tokenizer, model = load_tinyllama()
    device = "cpu"
    model = model.to(device)

    # Chat history
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    # User question
    user_input = st.text_input("üí¨ Ask a question", placeholder="e.g., What is deep learning?")
    if st.button("üîç Get Answer") and user_input.strip() != "":
        # Semantic search
        q_embedding = embedder.encode([user_input])
        D, I = faiss_index.search(np.array(q_embedding), k=1)
        context = chunks[I[0][0]]

        # Prompt format for TinyLlama
        prompt = f"""<|system|>You are a helpful AI tutor. Based only on the context, answer in at least 300 words.<|end|>
<|user|>Context:\n{context}\n\nQuestion: {user_input}<|end|>
<|assistant|>"""

        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512).to(device)
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=700,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1,
                do_sample=True,
                eos_token_id=tokenizer.eos_token_id
            )

        full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = full_output.split("<|assistant|>")[-1].strip()
        st.session_state.chat_history.append((user_input, response))

    # Show chat history
    for q, a in st.session_state.chat_history[::-1]:
        with st.chat_message("user"):
            st.markdown(f"**You:** {q}")
        with st.chat_message("assistant"):
            st.markdown(f"**TinyLlama:** {a}")
